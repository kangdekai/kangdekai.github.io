<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="视其所以"><title>YOLO9000论文阅读笔记(一) | 至止</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">YOLO9000论文阅读笔记(一)</h1><a id="logo" href="/.">至止</a><p class="description">视其所以，观其所由，察其所安。</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> archives</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">YOLO9000论文阅读笔记(一)</h1><div class="post-meta">Aug 3, 2017<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#摘要"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#一、背景"><span class="toc-number">2.</span> <span class="toc-text">一、背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、更好"><span class="toc-number">3.</span> <span class="toc-text">二、更好</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#批量归一（Batch-Normalization）"><span class="toc-number">3.1.</span> <span class="toc-text">批量归一（Batch Normalization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#高分辨率分类器"><span class="toc-number">3.2.</span> <span class="toc-text">高分辨率分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用锚窗（Anchor-Boxes）卷积"><span class="toc-number">3.3.</span> <span class="toc-text">使用锚窗（Anchor Boxes）卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#维度聚类"><span class="toc-number">3.4.</span> <span class="toc-text">维度聚类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#直接坐标预测"><span class="toc-number">3.5.</span> <span class="toc-text">直接坐标预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#细粒度特征（Fine-Grained-Features）"><span class="toc-number">3.6.</span> <span class="toc-text">细粒度特征（Fine-Grained Features）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多尺度训练"><span class="toc-number">3.7.</span> <span class="toc-text">多尺度训练</span></a></li></ol></li></ol></div></div><div class="post-content"><p>CVPR 2017刚刚结束，心想选几篇论文拜读一下，大牛就是大牛，仅供瞻仰。</p>
<p><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="external">YOLO9000: Better, Faster, Stronger</a> 由华盛顿大学艾伦研究所的大牛<a href="https://pjreddie.com/publications/" target="_blank" rel="external">Josehp Redmon</a>、<a href="https://homes.cs.washington.edu/~ali/" target="_blank" rel="external">Ali Farhadi</a>共同完成，获得CVPR 2017 最佳论文鼓励奖。</p>
<p>YOLO9000对YOLO模型进行改进，速度、精确度均有提升，提出了一种将分类与检测训练过程相结合的方法，使模型可以同时接受分类与检测的数据。同时针对不同分类、检测数据集数量级相差较大、类别粒度不一致的情况，提出WordTree的数据集融合方法，实现了YOLO模型的又一次飞跃，指导借鉴意义重大。</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ul>
<li>YOLO9000使用了一种<strong>新的、多尺度的训练方法</strong>，可以对多种大小的图像进行识别，能够在速度与精确度之间进行简单权衡。<br>在VOC2007数据集上，YOLOv2帧率为67FPS情况下的mPA为76.8，帧率为40FPS情况下的mPA为76.8，相比Faster RCNN和SSD精确度更高，速度更快。</li>
<li>文章提出了一种将<strong>目标检测与分类的训练相结合</strong>的方法。使用该方法YOLO9000能够同时在COCO目标检测数据集及ImageNet分类数据集上进行训练。该种训练结合方法能够使模型对没有检测标签（detection labels）的物体类别进行检测，并使用ImageNet检测任务对该方法进行验证。<br>YOLO9000在ImageNet检测任务上，200个类中仅有40个类有检测标签的情况下，取得了19.7的mPA。在156个类别不在COCO数据集中取得了16.0的mPA。相比YOLO仅能检测200个类别，YOLO9000能够在保证实时的情况下，检测多于9000个类别的物体。</li>
</ul>
<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>通用目标检测应该满足<strong>快、精确、识别种类广泛</strong>的要求。神经网络的应用提高了检测架构的速度和准确度，但大多数方法能够识别物体的种类非常有限。</p>
<p>当前目标检测的数据集相比其他任务，如分类和标识（tagging）非常有限。最常见的检测数据集只包含几百个标签的十万、或数百万的图片。而分类数据集通常含有成千上万的物体类别及数百万的图片。</p>
<p>我们希望检测数据集能够达到分类数据集的规模。然而检测数据集的标记相比分类或标识任务更加耗时耗力（标识通常由用户免费提供）。因此，近期不太可能出现与分类任务相同规模的检测数据集。</p>
<p>文章提出了一种新的方法以利用大量的分类数据来拓展检测系统的识别范围。文章中使用了一种物体类别的层次化视图，<strong>实现差异较大的数据集的融合</strong>。</p>
<p>文章还提出了一种将<strong>分类与检测相结合训练方法</strong>。该方法使用标记好的检测数据实现目标的精确定位，利用分类数据提高其识别范围及稳健性。</p>
<p>文章利用该方法对YOLO9000进行了训练，使其能够检测超过9000种不同物体。首先，文章对YOLO检测系统进行改进得到YOLOv2—当前最佳的实时目标检测器。之后利用数据集融合及结合训练的方法对模型在超过9000个类的ImageNet分类数据集及COCO检测数据集进行训练。</p>
<p>所有代码及预训练的模型可以在<a href="http://pjreddie.com/yolo9000/找到。" target="_blank" rel="external">http://pjreddie.com/yolo9000/找到。</a></p>
<h1 id="二、更好"><a href="#二、更好" class="headerlink" title="二、更好"></a>二、更好</h1><p>YOLO与RCNN等基于预选框的方法召回率较低、定位误差较大。</p>
<p>计算机视觉通常趋向于使用更大、更深的网络，更优的性能经常意味着训练更大的网络、集合多种模型。对于YOLOv2，我们希望维持速度基本不变的情况下提高其准确率。与扩大网络规模相反，我们对网络进行精简，使表征更容易学习。我们根据以往的设计经验，提出一种新的概念，以提升YOLO的性能，总结如表2。</p>
<h2 id="批量归一（Batch-Normalization）"><a href="#批量归一（Batch-Normalization）" class="headerlink" title="批量归一（Batch Normalization）"></a>批量归一（Batch Normalization）</h2><p>批量归一能够显著地提升网络的收敛。在YOLO的每个卷积层之后添加块正则化之后，mPA提升了2%。批量归一同样能够正则化模型，从而在不使用丢弃层（dropout）的情况下防止过拟合。</p>
<h2 id="高分辨率分类器"><a href="#高分辨率分类器" class="headerlink" title="高分辨率分类器"></a>高分辨率分类器</h2><p>当前所有目标检测方法均基于在ImageNet预训练的分类器。从AlexNet开始，大多数分类器的输入都小于256x256。原始的YOLO在224x224分辨率下训练网络，并在检测时将分辨率提升到448。这意味着网络必须同时学习目标检测及调整适应新的分辨率。</p>
<p>对于YOLOv2，首先在ImageNet上以448x448的分辨率对分类器微调十次（epoch），使分类器适应高分辨率的输入。对网络微调使其适应检测任务。高分辨率的分类网络提升了4%的mPA。</p>
<h2 id="使用锚窗（Anchor-Boxes）卷积"><a href="#使用锚窗（Anchor-Boxes）卷积" class="headerlink" title="使用锚窗（Anchor Boxes）卷积"></a>使用锚窗（Anchor Boxes）卷积</h2><p>YOLO使用卷积特征提取器之上的全连接层直接预测边界框。Faster RCNN使用人工选择的先验窗口而非直接预测边框坐标。Faster RCNN使用区域生成网络（region proposal network，RPN）预测锚窗偏移及其自信度。由于预测层是卷积层，故而RPN在特征图的每个位置都预测了偏移量。预测偏移量相比预测坐标而言，使问题更加简化，易于网络学习。</p>
<p>我们将YOLO的全连接层去除，使用锚窗预测边框。首先，去掉一个池化层以产生高分辨率的输出。同时使网络输入减小为416而非480x480，使特征图大小为奇数，此时特征图中央只有一个单元。由于较大的物体通常在图像中心位置，因此中心的一个单元相比邻近的四个单元的预测效果更好，YOLO卷积层对输入图像的下采样比率为32，因此输出特征图大小为13x13。</p>
<p>我们并未对每个锚窗进行类别及似物性预测，而是将类别预测机制与空间位置预测相分离。与YOLO相同的是，对于某一物体，似物性依然预测真实值与预测框的IOU，而类预测则预测给定类的条件概率。</p>
<p>使用锚窗能够较小地提高准确率。YOLO在每幅图像只能预测98个窗口，而使用锚窗之后能够预测超过1000个窗口。锚窗使用前的mAP为69.5、召回率为81%，使用之后的mAP为69.2、召回率为88%。虽然mAP下降了一些，但召回率的提升意味着我们的模型还有较大的提升空间。</p>
<h2 id="维度聚类"><a href="#维度聚类" class="headerlink" title="维度聚类"></a>维度聚类</h2><p>在应用锚窗的过程中，我们遇到了两个问题。第一个是窗口维度是手动选择的，虽然网络能够学习并调整窗口大小，但是合适的先验值更易于网络预测。</p>
<p>除了手动选择先验值，我们对训练集边框进行K均值（k-means）聚类以寻找合适的先验窗口大小。如果使用标准K均值，欧拉距离较大的窗口将会比小窗口产生更大的误差，由于我们想要能够产生更好IOU得分的先验窗口，与窗口大小无关。因此使用下列距离度量函数：<br>$$d(box,centroid)=1-IOU(box,centroid)$$<br>选择不同的K值，计算最临近中心框的IOU。文章选择K=5使模型复杂度和召回率之间可以平衡。类中心框与手动选择的锚窗有着明显不同，大多数为较高瘦的、而非矮宽的窗口。</p>
<h2 id="直接坐标预测"><a href="#直接坐标预测" class="headerlink" title="直接坐标预测"></a>直接坐标预测</h2><p>使用锚窗预测的第二个问题便是，模型在早期迭代过程中较不稳定。较多的不稳定来源于窗口预测的位置$(x,y)$。在区域生成网络中，网络预测$t_x$、$t_y$，使用下式计算$(x,y)$中心坐标：</p>
<p>$$x=(t_x<em>w_a)-x_a y=(t_y</em>w_b)-y_a$$</p>
<p>例如，$t_x=1$ 时，窗口将会右移锚窗宽度，而$t_x=-1$则会将窗口左移锚窗宽度。</p>
<p>该公式不具有约束性，锚窗可能在图像任意位置结束，无论预测窗口的位置。从模型的随机初始化将会消耗大量时间以稳定预测敏感的偏移。</p>
<p>与YOLO相同，模型预测相对单元格的位置坐标。真实值的范围在0到1之间变动。文章使用逻辑激活将网络预测值约束到该范围内。</p>
<p>该网络在输出特征图的每个单元预测5个边框。对于每个边框预测5个坐标值，$t_x$、$t_y$、$t_w$、$t_h$、$t_o$。若单元相对图像左上角的偏移为$(c_x,c_y)$，并且窗口的先验宽高为$p_w$、$p_h$，那么其窗口坐标预测值为：<br>$<br>b_x=\delta (t_x)+c_x \<br>b_y=\delta (t_y)+c_y \<br>b_w=p_w e^{t_w} \<br>b_h=p_h e^{t_h} \<br>Pr(object)*IOU(b,object)=\delta (t_o)<br>$</p>
<p>添加位置预测的约束之后，参数学习更加容易/模型更加稳定。维度聚类及边框中心位置约束相比YOLO提高了5%的准确率。</p>
<h2 id="细粒度特征（Fine-Grained-Features）"><a href="#细粒度特征（Fine-Grained-Features）" class="headerlink" title="细粒度特征（Fine-Grained Features）"></a>细粒度特征（Fine-Grained Features）</h2><p>改进版YOLO模型使用13x13的特征图检测目标。这对于大目标检测是足够的，利用细粒度特征以定位较小的物体。Faster RCNN和SSD都在多种特征图上运行网络从而获得一定范围的分辨率。文章采用一种不同的方式，只添加一个从低端26x26分辨率产生特征的遍历层（passthrough layer）。</p>
<p>遍历层通过堆叠不同通道中相邻的特征而非空间位置实现融合高低分辨率的特征，与ResNet的一致映射（identity mapping）相类似。该方法将26x26x512的特征图映射为13x13x2048的特征图，进而使其与原始特征进行融合。文中提出的检测器运行在这种方法拓展的特征图上，以获取细粒度特征，提高了将近1%的性能。</p>
<h2 id="多尺度训练"><a href="#多尺度训练" class="headerlink" title="多尺度训练"></a>多尺度训练</h2><p>原始YOLO输入分辨率为448x448。由于加入锚窗，分辨率变为416x416。由于模型中只使用了卷积层与池化层，使其能够在运行时缩放。为了提高YOLOv2的对不同大小输入的稳健性，作者使用该方法训练模型。</p>
<p>作者并未固定输入图像大小而是每隔几次迭代便更改网络。每隔10批，网络就会随机选择输入图像的尺寸。由于模型降采样倍数为32，故而作者选择32倍数的尺寸：${320,352,…,608}$，网络最小选择为320x320，最大为608x608。每次更换尺寸后，都对网络进行缩放，继续训练。</p>
<p>这种机制强制网络对多种尺度输入优化预测。这意味同一网络能对多种尺度目标进行检测。YOLOv2网络在较小的尺度上运行较快，这可以使用户在准确度和速度之间进行权衡。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2017/08/03/yolo9000/" data-id="cj5w2u1au0001ey5mqkyeb7yk" class="article-share-link">分享</a><div class="tags"><a href="/tags/深度学习/">深度学习</a></div><div class="post-nav"><a href="/2017/07/29/hello-world/" class="next">Hello World</a></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://yoursite.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/03/yolo9000/">YOLO9000论文阅读笔记(一)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/29/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.csdn.net/u014593748" title="KangDK's CSDN" target="_blank">KangDK's CSDN</a><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="大道至简——Hexo简洁主题推荐" target="_blank">大道至简——Hexo简洁主题推荐</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">至止.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>