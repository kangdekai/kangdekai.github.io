<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="视其所以"><title>YOLO9000论文阅读笔记(二) | 至止</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">YOLO9000论文阅读笔记(二)</h1><a id="logo" href="/.">至止</a><p class="description">视其所以，观其所由，察其所安。</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> archives</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">YOLO9000论文阅读笔记(二)</h1><div class="post-meta">Aug 3, 2017<span> | </span><span class="category"><a href="/categories/论文笔记/">论文笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#更快"><span class="toc-number">1.</span> <span class="toc-text">更快</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Darknet-19"><span class="toc-number">1.1.</span> <span class="toc-text">Darknet-19</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类过程训练"><span class="toc-number">1.2.</span> <span class="toc-text">分类过程训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#检测过程训练"><span class="toc-number">1.3.</span> <span class="toc-text">检测过程训练</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#更强"><span class="toc-number">2.</span> <span class="toc-text">更强</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#层次分类"><span class="toc-number">2.1.</span> <span class="toc-text">层次分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用WordTree结合数据集"><span class="toc-number">2.2.</span> <span class="toc-text">使用WordTree结合数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类与检测结合"><span class="toc-number">2.3.</span> <span class="toc-text">分类与检测结合</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-结论"><span class="toc-number">3.</span> <span class="toc-text">5. 结论</span></a></li></ol></div></div><div class="post-content"><h1 id="更快"><a href="#更快" class="headerlink" title="更快"></a>更快</h1><p>速度与准确率对于检测任务都是非常重要的。大多数的检测任务，如机器人或自动驾驶汽车均依赖于低延迟的预测。</p>
<p>大多数检测架构均将VGG-16作为基础特征提取器。VGG-16是一种强大的、精确的分类网络，但同样存在大量冗余。对于一副224x224的图像，VGG-16的卷积层迭代一次就需要306.9亿次浮点运算。</p>
<p>YOLO架构使用基于GoogleNet的经典架构，该网络比VGG-16更快，前向传播仅需85.2亿次浮点操作，然而其精确度略差于VGG-16。在ImageNet数据集上、输入为224x224情况下，YOLO模型的5准确率为88.0%，而VGG-16为90.0%。</p>
<h2 id="Darknet-19"><a href="#Darknet-19" class="headerlink" title="Darknet-19"></a>Darknet-19</h2><p>作者提出一种新的分类模型作为YOLOv2的基础。与VGG类似，作者使用最常见的3x3的滤波器，在每次池化之后对通道数加倍。借鉴NIN（Network in Network）的经验，作者使用全局平均池化预测结果，同时利用1x1滤波器压缩3x3卷积层间的特征表示。使用批归一话（Batch Normalization）稳定训练过程，加速收敛，正规化模型。</p>
<p>最终的模型Darknet-19有19个卷积层，5个最大池化层。Darknet-19仅需55.8亿次浮点运算，在ImageNet上的前一准确率为72.9%，前五准确率为91.2%。</p>
<h2 id="分类过程训练"><a href="#分类过程训练" class="headerlink" title="分类过程训练"></a>分类过程训练</h2><p>作者在标准ImageNet 1000个类的数据集上训练网络160代（epochs），使用随机梯度下降方法、学习率初始化为0.1，以4次多项式方式递减，权重衰减系数为0.0005，动量为0.9。使用Darknet神经网络架构（Darknet neural network）。训练过程使用标准的数据扩增方式包括随机取块、旋转、色调、饱和度、曝光度等通道偏移。</p>
<p>开始时在224$\times$224的模型上训练，之后在更大的网络上微调。微调时仍然使用以上系数，但只训练10代，学习率初始值为$10^{-3}$。</p>
<h2 id="检测过程训练"><a href="#检测过程训练" class="headerlink" title="检测过程训练"></a>检测过程训练</h2><p>将分类网络最后的卷积层换为含1024个滤波器的3x3卷积层，每层之后是一个1x1的卷积层，其大小为输出参数数量。对于VOC，每个窗口需要预测5个框，每个框5个坐标值，共有20种类别可能，因此是125大小的滤波器。同样在最后与倒数第二层卷积层之间添加一个3x3x512的穿透层（passthrough layer）以获取细粒度特征。</p>
<p>网络经过160代训练、学习率初始值为$10^{-3}$，在第10、60、90代降低学习率。权值衰减为0.0005、动量为0.9。数据扩增方式和YOLO和SSD相同，随机取块、颜色偏移等。COCO和VOC上的训练策略相同。</p>
<h1 id="更强"><a href="#更强" class="headerlink" title="更强"></a>更强</h1><p>作者提出了分类与检测数据相结合的训练方法。该方法使用检测标签的数据学习检测相关的信息，如边框坐标、似物性及分类常见物体。使用分类标签的图像拓展模型的识别范围。</p>
<p>训练过程中，分类与检测标签图像混合使用。当网络输入检测标签图像时，就基于全YOLOv2损失函数进行反向传播。输入分类标签图像时，仅对分类相关结构反向传播。</p>
<p>但该方法遇到一些挑战。检测数据集只包含常见物体和常用标签，如“狗”或“船”。而分类数据集的标签范围则更深更宽。ImageNet含有100多品种的狗，包括“诺福克犬”，“约克郡梗犬”和“贝灵顿犬”。如果我们要训练两个数据集，就需要一种统一的方式来合并这些标签。</p>
<p>大多数分类方法使用softmax层对所有可能类别来计算最终的概率分布。softmax的预设是<strong>类间互斥</strong>，这给数据集融合带来一定问题，例如，该模型下，ImageNet和COCO将难以融合，因为类“诺福克犬”和“狗”并非互斥的，当然也可以使用多标签模型来组合非互斥的数据集但这就忽略了已知数据之间的结构，例如，COCO中所有类都是互斥的。</p>
<h2 id="层次分类"><a href="#层次分类" class="headerlink" title="层次分类"></a>层次分类</h2><p>ImageNet标签来自WordNet，这是一个包含词间关系的结构化语言数据库。在WordNet中，“诺福克犬”和“约克夏梗”是“狗”的下位词，属于“猎犬”，是一种“狗”，“犬”的一种等等。大多数分类方法都假定扁平的类间关系，而结构化的类间关系却是必须。</p>
<p>由于语言的复杂性，WordNet采用了有向图，而不是树状结构，例如，“狗”是“犬”和“家畜”的下义词。为了简化问题，作者将ImageNet中的概念构建为层次树而非图结构以描述问题。</p>
<p>树的构建中，作者检查了ImageNet中的视觉名词，并通过WordNet图查看其路径根节点，该情况下是“物体”。许多同义词在图中只有一条通路，所以首先添加这种了路径到树中。之后迭代地检查剩下的概念，使添加的路径达到最少。所以如果一个概念有两条通向根的路径，一路需向树中添加三条边，而另一路只需增加一条边，那么选择最短的路径。</p>
<p>最终产生WordTree，视觉概念的层次化模型。使用WordTree分类过程中，在给定该同义词的情况下，针对每个节点的每个下位词的概率来预测每个节点的条件概率。例如，预测“狗”节点时：<br>$$<br>P_r(诺福克犬|狗) \\<br>Pr(约克郡梗犬|狗) \\<br>Pr(贝灵顿犬|狗) \\<br>…<br>$$</p>
<p>如果要计算一个特定节点的绝对概率，那只需通过树的路径根节点并乘以条件概率。所以如果想知道一张图像是否包含诺福克犬，只需计算：<br>$$<br>P_r(诺福克犬) = P_r(诺福克犬|狗) \\<br>∗P r(狗\ | \ 猎犬) \\<br>∗…∗ \\<br>∗P_r(哺乳动物|动物) \\<br>∗P_r(动物|实物)<br>$$</p>
<p>分类时，假设该图像包含一个实物：$P_r(物体)= 1$。为了验证该方法，作者使用1000类的ImageNet构建的WordTree上训练了Darknet-19模型。为了构建WordTree 1K，作者将所有中间节点中添加到WordTree中，将标签空间从1000扩展到1369。在训练期时，将真实标签在树上传播，若图像被标记为“诺福克犬”，它也会被标记为“狗”和“哺乳动物”等。为了计算条件概率，模型预测了1369个值的向量，计算同一概念所有下位词的softmax值。</p>
<blockquote>
<p>注：从“物体”根节点到底层节点的路径上，可能有某些词语不是类标签，如“物体”–&gt;“动物”–&gt;“哺乳动物”–&gt;“狗”，其中“动物”、“哺乳动物”可能不在原始标签之中，故需要对标签空间进行拓展。</p>
</blockquote>
<p>使用与之前相同的训练参数，我们的层次Darknet-19达到了71.9％的前1个精度<br>90.4％的前5个精度。尽管增加了369个附加概念，使网络预测一个树状结构，网络准确性略有下降，<strong>但这种分类适用于新的或未知的对象类别</strong>。例如，如果网络输入狗的图片，但不确定是什么类型的狗，它仍然会较高置信度地预测“狗”，而其下位词的预测自信度较低。</p>
<p>该机制同样可以用于检测。使用YOLOv2物体检测器预测$P_r(物体)$的值。检测器预测一个边界框和树节点的概率。从根节点对树进行便利，在节点处选择自信度最高的路径，直到达到一定阈值，则预测为该类对象。</p>
<h2 id="使用WordTree结合数据集"><a href="#使用WordTree结合数据集" class="headerlink" title="使用WordTree结合数据集"></a>使用WordTree结合数据集</h2><p>使用WordTree可以合理地将多个数据集合并，能简单地将数据集的类别映射到树上的词项。图6显示了使用WordTree可以组合来自ImageNet和COCO的标签。WordNet非常多样化，所以在大多数据集上均可使用这种技术。</p>
<h2 id="分类与检测结合"><a href="#分类与检测结合" class="headerlink" title="分类与检测结合"></a>分类与检测结合</h2><p>既然已经能够使用WordTree融合数据集，就可以对分类与检测结合的模型进行训练。为了训练一个非常大的检测器，作者使用COCO检测数据集和ImageNet发布的其前9000个类。为了评估该方法，同样加入未曾添加的ImageNet检测任务中的类别。所得数据集相应的WordTree共有9418类。ImageNet是一个较大的数据集，因此作者通过对COCO进行过采样来平衡数据集，使ImageNet只有4比1的比例。</p>
<p>使用该数据集对YOLO9000进行训练。基于YOLOv2架构，将其输出的5个先验（priors）改为3个（？？？）。网络输入检测图像时，就进行正常的反向传播。对于分类损失，只对其标签及其上层标签进行反向传播。例如，如果标签是“狗”，将会给出错误以提高其预测准确率。</p>
<p>输入分类图像时，只会反向传播分类损失。为了做到这一点，只需找到预测该类概率最高的边界框仅在其预测的树上计算损失。同时作者也假设预测的框重叠0.3 IOU以上的是真实物体的框，并且反向传播其似物性损失。</p>
<p>使用该种联合训练方式，YOLO9000使用COCO的检测数据学习预测物体类别及其位置，使用ImageNet数据学习预测更大范围的物体。</p>
<p>作者在ImageNet检测任务上对YOLO9000做出评估。ImageNet与COCO的检测任务有44个类别相同，这意味着YOLO9000训练集中分类图像占主要地位。 YOLO9000整体mAP准确率为19.7 mAP，而对于不相交的156对象类，网络没有学习过它们的检测数据，其mAP准确率为16.0。这要比DPM准确率更高，而YOLO9000只是在不同数据集上半监督地训练。该模型可以实时地检测9000多个类别。</p>
<p>当分析YOLO9000在ImageNet上的表现时，可以看出模型能够很好地学习动物的新品种，但是类别（如服装和设备）之间难以区分。新的动物更容易学习，因为物体预测从COCO中的动物得到很好的泛化。相反，COCO只有“人”，而有任何衣服类型的边框，所以YOLO9000在“太阳眼镜”或“游泳裤”等类别时显得困难。</p>
<h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h1><p>该介绍了YOLOv2和YOLO9000实时检测系统。 YOLOv2是最先进、最快、跨越多种数据集的检测系统。此外，它可以输入各种大小的图片，提供了速度和精度之间的平滑折衷。</p>
<p>YOLO9000是一个实时检测框架，通过检测和分类过程的结合，能够检测超过9000个物体类别。作者使用WordTree来组合数据各种来源和使用融合优化技术使模型同时在ImageNet和COCO上训练。 YOLO9000是一个突破检测和分类之间的数据集大小差距的有力之举。</p>
<p>文章的许多技术可以泛化到目标检测之外。ImageNet WordTree表示提供了一个更丰富、更详细的图像分类输出空间。使用层次分类的数据集组合在分类和分割域中非常有用。诸如多尺度训练等技巧可以使各种视觉任务从中获益。</p>
<p>未来的工作中，作者希望将类似的技术应用于<strong>弱监督图像分割</strong>。通过训练过程中将弱标签分配给分类数据，使用更强大的匹配策略等来改进检测结果。计算机视觉需要拥有大量标签数据。作者希望能够寻找融合不同来源和数据结构的方法共同打造更强大的视觉世界。</p>
<hr>
<p>全剧终</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2017/08/03/YOLO9000-2/" data-id="cj5z8z42f0000ny5mkpshodvl" class="article-share-link">分享</a><div class="tags"><a href="/tags/深度学习/">深度学习</a></div><div class="post-nav"><a href="/2017/08/02/yolo9000/" class="next">YOLO9000论文阅读笔记(一)</a></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://yoursite.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/论文笔记/">论文笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/03/YOLO9000-2/">YOLO9000论文阅读笔记(二)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/02/yolo9000/">YOLO9000论文阅读笔记(一)</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://blog.csdn.net/u014593748" title="KangDK's CSDN" target="_blank">KangDK's CSDN</a><ul></ul><a href="https://www.haomwei.com/technology/maupassant-hexo.html" title="大道至简——Hexo简洁主题推荐" target="_blank">大道至简——Hexo简洁主题推荐</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">至止.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>